# Human Balance Analytics Project

## Project Overview

### Spark and Human Balance

In this project, we'll leverage the power of Spark and AWS Glue to process data from multiple sources, categorize it, and curate it for future queries. This builds upon your existing knowledge of Spark and AWS Glue, incorporating previously written code. You'll extend your skills to create curated step trainer data suitable for machine learning using AWS Glue jobs.

## Project Introduction: STEDI Human Balance Analytics

You'll step into the shoes of a data engineer working for the STEDI team. Your mission is to build a data lakehouse solution that houses sensor data, which will be used to train a machine learning model.

## Project Details

The STEDI team has developed the innovative STEDI Step Trainer:

* **Trains Users:** Guides users through a STEDI balance exercise.
* **Collects Sensor Data:** Utilizes built-in sensors to collect data for training a machine-learning algorithm to accurately detect steps.
* **Companion Mobile App:**  The Step Trainer pairs with a mobile app to gather customer data and interact with the device's sensors.

STEDI has received enthusiastic responses from millions of early adopters eager to purchase and use the Step Trainers. Many have already begun testing their balance with the device and mobile app.

**The Challenge:**

* The Step Trainer acts as a motion sensor, recording the distance of detected objects.
* The mobile app uses the phone's accelerometer to detect motion in the X, Y, and Z directions.
* The STEDI team aims to utilize this motion sensor data to train a machine learning model for real-time step detection.

**Privacy is Paramount:**

* Only data from early adopters who have explicitly agreed to share their data for research purposes will be used. This includes both Step Trainer and accelerometer data.

## Project Summary

As a data engineer on the STEDI Step Trainer team, your responsibilities are:

1. **Extract:** Obtain data generated by the STEDI Step Trainer sensors and the mobile app.
2. **Curate:**  Organize this data into a well-structured data lakehouse solution on AWS.

The goal is to prepare this data for data scientists to train the machine learning model for step detection.

## Project Environment

### AWS Environment

You'll leverage the data from the STEDI Step Trainer and mobile app to build a cloud-based lakehouse solution. This solution will curate the data for the machine learning model using the following AWS services:

* **Python and Spark:** For data processing and analysis.
* **AWS Glue:** To build ETL pipelines and data cataloging.
* **AWS Athena:** For interactive SQL queries on the data.
* **AWS S3:** To store the raw and curated data.

You'll be provided with a temporary AWS account with a $25 budget for this project. **Be mindful of the resources you create and their associated costs.**



### Workflow Environment Configuration

You'll primarily use AWS Glue and Glue Studio to create Python scripts. These tools offer web-based editors with options for writing or generating PySpark code. Remember to save any code developed or executed in these editors to your local Github repository.

Local Python editors can also be used, but keep in mind that Glue Jobs must be submitted to your AWS Glue environment for actual testing and execution.

## Project Data

STEDI provides three types of JSON data from the Step Trainer:

1. **Customer Records:** Information from fulfillment and the STEDI website.
   
   * **Fields:** `serialnumber`, `sharewithpublicasofdate`, `birthday`, etc. 

2. **Step Trainer Records:** Data from the motion sensor.
  
   * **Fields:** `sensorReadingTime`, `serialNumber`, `distanceFromObject`

3. **Accelerometer Records:** Data from the mobile app.

   * **Fields:** `timeStamp`, `user`, `x`, `y`, `z`

### Getting the Data

1. Go to the [nd027-Data-Engineering-Data-Lakes-AWS-Exercises](opens in a new tab) repository.
2. Click "Download Zip" and extract the files.
3. The JSON data files are in the `project/starter` subfolders.

**Expected Row Counts:**
* `customer_landing` table: 956 rows
* `accelerometer_landing` table: 81,273 rows
* `step_trainer_landing` table: 28,680 rows


## Project Instructions

Using AWS Glue, AWS S3, Python, and Spark, you'll create Python scripts to build a lakehouse solution that meets the following requirements:

1. **Create Landing Zones:** Set up S3 directories for `customer_landing`, `step_trainer_landing`, and `accelerometer_landing` and copy the data there.

2. **Create Glue Tables for Landing Zones:**  Write `customer_landing.sql` and `accelerometer_landing.sql` scripts to create Glue tables for the landing zones. Share these scripts in your Git repository.

3. **Query Tables:** Query the created tables using Athena and take screenshots of the results (`customer_landing.png` and `accelerometer_landing.png`).

4. **AWS Glue Jobs:** Create two Glue jobs:
   * **Job 1:** Sanitize customer data from the landing zone and store only those who agreed to share data in the trusted zone (`customer_trusted` table).
   * **Job 2:** Sanitize accelerometer data from the landing zone and store only readings from customers who agreed to share data in the trusted zone (`accelerometer_trusted` table).

5. **Verify Customer Data:**  Query the `customer_trusted` table in Athena and take a screenshot (`customer_trusted.png`).

6. **Address Data Quality Issue:** The fulfillment website had a bug with serial numbers. Write a Glue job to:
   * Sanitize customer data in the trusted zone.
   * Create a curated zone table (`customers_curated`) with only customers who have accelerometer data and agreed to share data.

7. **Glue Studio Jobs:** Create two Glue Studio jobs:
   * **Job 1:** Read Step Trainer data from S3 and populate a trusted zone table (`step_trainer_trusted`) with data from customers in `customers_curated`.
   * **Job 2:** Create an aggregated table (`machine_learning_curated`) combining Step Trainer readings and accelerometer data for the same timestamp, only for customers who agreed to share data.

